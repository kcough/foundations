{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kate Cough\n",
    "\n",
    "#### June 29 2017\n",
    "\n",
    "\n",
    "## Supreme Court Project Guide\n",
    "\n",
    "The ultimate goal of this project is to build a database of Supreme Court cases for 2016 that includes the dialogue from the oral arguments of each case. As we have seen in class the arguments were scraped from this page: https://www.supremecourt.gov/oral_arguments/argument_transcript.aspx \n",
    "\n",
    "I have already downloaded and transformed the PDFs of the transcripts into text documents which you can download from courseworks: supreme_court_pdfs_txt.zip\n",
    "\n",
    "There are three steps that you need to complete:\n",
    "\n",
    "**Please note:** Step 3 is the most challenging--if you want to spend some time coding, you can skip Steps 1 and 2 and get to work on Step 3\n",
    "\n",
    "**STEP 1:** scrape all of the case information available on this page: https://www.supremecourt.gov/oral_arguments/argument_transcript.aspx \n",
    "\n",
    "This should include case name, docket number, etc--and most importantly the name of the PDF file. All of the text files share the exact same name as the PDF files they came from. This file name will allow you to connect your transcription data with your case data. \n",
    "\n",
    "It is up to you what kind Data structure you want to build. But it likely to be a list of lists, or list of dictionaries--for each case you will have a list or dictionary of the information you scrape from the webpage.\n",
    "\n",
    "**STEP 2:** find a secondary source to scrape/integrate with your case data. The information on the Supreme Court page is very limited. You need to find a source or group of sources that ad information. The most important information would likely be: the decision, who voted for and against, and the state of origin of the case (for geocoding). You might think of other great things to put in there too! This information needs to be merged with the data you have from STEP 2.\n",
    "\n",
    "**STEP 3:** use regular expressions to clean up and parse the text files so that you have a searchable data structure containing the dialog from the transcripts. \n",
    "\n",
    "From a data architecture perspective, you probably want to have a separate list for each case and in each list a data structure that pairs the speaker with what she/he says. Like:\n",
    "\n",
    "`[['MR. BERGERON',\" Yes. That's essentially the same thing\"],[ 'JUSTICE SOTOMAYOR',' So how do you deal with Chambers?']]`\n",
    "\n",
    "This is a list of lists --it could also be a list of dictionaries if you want it to be. The real programmatic challenge here is to clean up the text files and parse them successfully. Most of the instructions below are devoted to this, but Steps 1 and 2 are also extremely important.\n",
    "\n",
    "Go step-by-step through this, and email me whenever you get stuck, and I will help. If you complete all the steps before Tuesday, email me if you want to go further.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1\n",
    "Scrape all of the necessary information from:\n",
    "\n",
    "https://www.supremecourt.gov/oral_arguments/argument_transcript.aspx \n",
    "\n",
    "You should result and a list of dictionaries for each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Import your libraries and all other things\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Write your scraping code here\n",
    "url = 'https://www.supremecourt.gov/oral_arguments/argument_transcript.aspx'\n",
    "raw_html = urlopen(url).read()\n",
    "doc = BeautifulSoup(raw_html, 'html.parser')\n",
    "\n",
    "result = requests.get('https://www.supremecourt.gov/oral_arguments/argument_transcript.aspx')\n",
    "#store the result of in the variable 'result'\n",
    "# result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print out your list of lists or dictionaries here\n",
    "#define the variable doc\n",
    "#the info is in the table (defined by the class table datatables) and in the table rows.\n",
    "#define the variable for the table rows\n",
    "#make a list to store the info\n",
    "\n",
    "table = doc.find('table', class_ = 'table datatables')\n",
    "cases = table.find_all('tr')\n",
    "\n",
    "supreme_court_list_all = []\n",
    "\n",
    "for each_case in cases:\n",
    "    \n",
    "    current = {}\n",
    "    #make a dictionary. for each entry in the dictionary \n",
    "    #there will be four key : value pairs: link, name, date and docket_number, defined below\n",
    "    #using beautiful soup and the tags, we'll find each one. remember we're already inside the tr tag\n",
    "    \n",
    "    link = each_case.find_all('td')[0].find('a')\n",
    "    \n",
    "    name = each_case.find('span')\n",
    "    \n",
    "    date = each_case.find_all('td')[1].string\n",
    "    \n",
    "    docket_number = each_case.find_all('td')[0].find(target = '_blank')\n",
    "    \n",
    "    if link:\n",
    "        current['Text'] = link['href'].split('/')[-1]\n",
    "    if name:\n",
    "        current['Case Name'] = name.string\n",
    "    if date:    \n",
    "        current['Date Argued'] = date\n",
    "    if docket_number:\n",
    "        current['Docket Number'] = docket_number.string.strip()\n",
    "    \n",
    "    supreme_court_list_all.append(current)\n",
    "    \n",
    "supreme_court_list_all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_supreme_court = pd.DataFrame(supreme_court_list_all)\n",
    "scraped_supreme_court.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.to_csv(\"supreme_court_list_all.csv\", index=False)\n",
    "# supreme_court_list_all = pd.read_csv('supreme_court_list_all.csv')\n",
    "# supreme_court_list_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_supreme_court.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scraped_supreme_court.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = scraped_supreme_court['Text'].unique()\n",
    "array.sort()\n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Open a text file from your computer\n",
    "f = open('/Users/kaitlincough/Documents/Lede/thirkield/final_project_supreme_court/pdfs/15-777_1b82.txt', 'r')\n",
    "sample_transcript = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !cat /Users/kaitlincough/Documents/Lede/thirkield/python_notebooks_thirkield/pdfs/15-777_1b82.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a look at the text file\n",
    "sample_transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning comes first\n",
    "\n",
    "A step-by-step way of Cleaning up this mess.\n",
    "\n",
    "Step 1. You might notice that every page has:\n",
    "\n",
    "`Alderson Reporting Company\n",
    "\n",
    "\f",
    "Official - Subject to Final Review`\n",
    " \n",
    "You want to get rid of that. I would use a regex sub() \n",
    "\n",
    "Step 2. **Line Numbers:** you might also notice these annoying line numbers going from 1 - 25 everywhere: I would use the regex sub() to get rid of this too -- but be very careful, you don't want to get rid of all the numbers in there. The cleaning doesn't have to be perfect, but try to get as many of them as you can without deleting other numbers.\n",
    "\n",
    "Step 3 and 4. **chop off the beginning/ chop off the end**: now it would be very helpful to get rid of all of the text that comes before the arguments begins, and all the text that comes after the argument (each page has a really annoying index at the end that you don't want to be searching through). Look for words or phrases that uniquely repeat at the beginning and at the end of the arguments. The easiest way to isolate this, to do a simple split() on one of those phrases, and keep the half of The split you want. (Am I being too cryptic here?--a good split should give you list with two elements when you want to keep one of them) Think about it and email me.\n",
    "\n",
    "Try to get these 4 cleaning actions to work step-by-step in the 4 cells below. As you go, I would assign each cleaner version of the text to a new variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting rid of the beginning\n",
    "remove_beginning = re.split(r'\\bPROCEEDINGS \\d \\(\\d\\d\\:\\d\\d \\w\\.m\\.\\)', sample_transcript)\n",
    "remove_beginning[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting rid of the Alderson Reporting Company\n",
    "remove_alderson = re.sub('Alderson Reporting Company|Official - Subject to Final Review', '', remove_beginning[1])\n",
    "remove_alderson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_end = re.split(r'Whereupon', remove_alderson)\n",
    "remove_end[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the number on the left hand side of the page\n",
    "remove_numbers = re.sub(r'[\\n ][12]?\\d |\\n\\n\\n|', '', remove_end[0])\n",
    "remove_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove more numbers\n",
    "remove_nums = re.sub(r\" \\d \", \"\", remove_numbers)\n",
    "remove_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the katherine sullivan line\n",
    "remove_argument = re.sub(r\"\\w+ ARGUMENT [^a-z]+ (PETITIONER|RESPONDENT)S?\", \"\", remove_nums)\n",
    "remove_argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of the x0c\n",
    "remove_x0 = re.sub(r'[\\x0c]*', '', remove_argument)\n",
    "remove_x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and the ns\n",
    "remove_n = re.sub(r'\\n', '', remove_x0)\n",
    "remove_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the timestamp before roberts\n",
    "remove_time = re.split(r\"([A-Z.\\s]+:)\", remove_n)\n",
    "remove_time[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_time = re.split(r\"([A-Z.\\s]+:)\", remove_argument)\n",
    "remove_time[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get your dialogue list\n",
    "Now this transcription should be clean enough to get a list with every speaker, and what the speaker said. The pattern for the speakers is fairly obvious--my recommendation is to do a split using groups (like the example I show above with \"tomorrow and tomorrow\").\n",
    "\n",
    "If you write your regular expression correctly: you should get a single list in which each element is either a speaker, or what was said."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of speaker and speech\n",
    "\n",
    "speech1 = remove_time[1:]\n",
    "speech1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make it a list of pairs\n",
    "If you got your list the way I recommended to, it is just single list with elements after element--you need to figure out how to change it so you pair the speaker with what is said. Give it some thought, there are a few ways to try to do this. If you made it this far, you're doing great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make it a list of pairs of speaker and speech\n",
    "\n",
    "speech2 = list(zip(speech1[0::2], speech1[1::2]))\n",
    "speech2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_beginning = re.split(r'\\bPROCEEDINGS \\d \\(\\d\\d\\:\\d\\d \\w\\.m\\.\\)', sample_transcript)\n",
    "remove_alderson = re.sub('Alderson Reporting Company|Official - Subject to Final Review', '', remove_beginning[0])\n",
    "remove_end = re.split(r'Whereupon', remove_alderson)\n",
    "remove_numbers = re.sub(r'[\\n ][12]?\\d |\\n\\n\\n|', '', remove_end[0])\n",
    "remove_nums = re.sub(r\" \\d \", \"\", remove_numbers)\n",
    "remove_argument = re.sub(r\"\\w+ ARGUMENT [^a-z]+ (PETITIONER|RESPONDENT)S?\", \"\", remove_nums)\n",
    "remove_x0 = re.sub(r'[\\x0c]*', '', remove_argument)\n",
    "remove_n = re.sub(r'[\\n]', '', remove_x0)\n",
    "remove_time = re.split(r\"([A-Z.\\s]+:)\", remove_n)\n",
    "remove_time[1:]\n",
    "speech1[2] = re.sub(r\"\\.\\s[A-Z.\\s]{67}\", \"\", speech1[2])\n",
    "speech1[-24] = re.sub(r\"\\.\\s[A-Z.\\s]{71}\", \"\", speech1[-24])\n",
    "speech2 = list(zip(speech1[0::2], speech1[1::2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def court_text(lines):\n",
    "    remove_beginning = re.split(r'\\bPROCEEDINGS \\d \\(\\d\\d\\:\\d\\d \\w\\.m\\.\\)', lines)\n",
    "    remove_alderson = re.sub('Alderson Reporting Company|Official - Subject to Final Review', '', remove_beginning[0])\n",
    "    remove_end = re.split(r'Whereupon', remove_alderson)\n",
    "    remove_numbers = re.sub(r'[\\n ][12]?\\d |\\n\\n\\n|', '', remove_end[0])\n",
    "    remove_nums = re.sub(r\" \\d \", \"\", remove_numbers)\n",
    "    remove_argument = re.sub(r\"\\w+ ARGUMENT [^a-z]+ (PETITIONER|RESPONDENT)S?\", \"\", remove_nums)\n",
    "    remove_x0 = re.sub(r'[\\x0c]*', '', remove_argument)\n",
    "    remove_n = re.sub(r'[\\n]', '', remove_x0)\n",
    "    remove_time = re.split(r\"([A-Z.\\s]+:)\", remove_n)\n",
    "    remove_time[1:]\n",
    "    speech1[2] = re.sub(r\"\\.\\s[A-Z.\\s]{67}\", \"\", speech1[2])\n",
    "    speech1[-24] = re.sub(r\"\\.\\s[A-Z.\\s]{71}\", \"\", speech1[-24])\n",
    "    speech2 = list(zip(speech1[0::2], speech1[1::2]))\n",
    "    \n",
    "    return speech2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "court_text(sample_transcript)\n",
    "#run the function on sample_transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through all texts\n",
    "If you made it this far--congratulations! \n",
    "The only thing left is to set up a loop that looks through all the texts and runs the cleanup and parsing when each one. You will need to have completed Step 1 in order to be able to do this loop because you will need the names to PDFs to do it. (Also each final list should also contain the PDF name, so you can reference it from your case database.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_cases = ['14-1538_j4ek', '14-9496_feah']\n",
    "supreme_court_list_all = []\n",
    "#create an empty list\n",
    "path = '/Users/kaitlincough/Documents/Lede/thirkield/final_project_supreme_court/pdfs'\n",
    "for file_name in array:\n",
    "    print(file_name)\n",
    "    # bad files\n",
    "    if file_name != '15-1358_7648' and file_name != '15-577_l64n' and file_name !='14-1055_h3dj' and file_name != '15-866_j426' and file_name != '16-32_mlho' and file_name!= '16-466_4g15' and file_name !='16-529_21p3':\n",
    "#         f = open(path + file_name + 'txt' + 'r')\n",
    "        sample_transcript = f.read()\n",
    "        this_list = court_text(sample_transcript)   \n",
    "#         remember court_text was our function, and it's being applied to sample_transcript\n",
    "        better_list = []\n",
    "        #create another empty list \n",
    "        for each in this_list:\n",
    "            entry = list(each)\n",
    "            entry.append(file_name)\n",
    "            better_list.append(entry)\n",
    "        this_list.append(file_name)\n",
    "        supreme_court_list_all.extend(better_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# supreme_court_list_all\n",
    "# col_names = ['speaker', 'words', 'case_id']\n",
    "# supreme_df = pd.DataFrame(supreme_court_list_all, columns=col_names)\n",
    "# supreme_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After this we can take the information from our scraped file and this loop through all the texts\n",
    "and merge them. There's also a third data frame with information gathered from elsewhere. It's called cases_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's look at the scraped file and remind ourselves what's in there\n",
    "scraped_supreme_court.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete Text column, clean up Docket Number so it'll match later\n",
    "scraped_supreme_court['Docket Number'].replace(r'\\.', '', regex=True, inplace=True)\n",
    "del scraped_supreme_court['Text']\n",
    "scraped_supreme_court.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the clean df with the information from looping through the texts\n",
    "supreme_df_clean = pd.read_csv('/Users/kaitlincough/Documents/Lede/thirkield/cases_data.csv')\n",
    "supreme_df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get rid of the last four digits at the end of case id so you can join it with the cases df\n",
    "supreme_df_clean['case_id'].replace(r'\\_\\w\\w\\w\\w', '', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases_clean_supreme = cases_df.merge(supreme_df_clean, left_on='Docket Number', right_on='case_id', how='outer')\n",
    "\n",
    "df_all_info = scraped_supreme_court.merge(supreme_df_clean, left_on='Docket Number', right_on='case_id', how='outer' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean up the columns a little, delete duplicates\n",
    "\n",
    "df_all_info.rename(columns={'words': 'Text', 'speaker': 'Speaker', 'Date Argued_x':'Date Argued', 'Area*': 'Area'}, inplace=True)\n",
    "\n",
    "df_all_info['Speaker'].replace(r'\\:', '', regex=True, inplace=True)\n",
    "\n",
    "#get rid of the case_id column, it's redundant\n",
    "del df_all_info['case_id']\n",
    "\n",
    "# del df_all_info['case_id']\n",
    "# del df_all_info['Date Argued_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's bring in the data frame we all created as a group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_df = pd.read_csv('/Users/kaitlincough/Documents/Lede/thirkield/final_project_supreme_court/supreme_court_info_created.csv')\n",
    "cases_df.rename(columns={'Area*': 'Area'}, inplace=True)\n",
    "\n",
    "cases_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the df we created (cases_df) with the df made after looping through all the texts and adding the scraped df--\n",
    "#so merge df_all_info with cases_df. The common column is Docket Number\n",
    "\n",
    "final_df = cases_df.merge(df_all_info, left_on='Docket Number', right_on='Docket Number', how='outer')\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df['Text'].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting the words in the text column and creating a new column with the count\n",
    "final_df['Word Count'] = final_df['Text'].apply(lambda x: len(x.split()))\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of the NA values\n",
    "\n",
    "final_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a DF with ONLY the Justices\n",
    "justices_only_df = final_df[final_df['Speaker'].str.contains('JUSTICE')]\n",
    "justices_only_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's get rid of the columns we don't need\n",
    "# justices_only_df.drop(['State', 'Date Argued_x', 'Decision', 'Status','Court Leaning', 'Previous Court', 'Date Argued_y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justices_only_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justices_only_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justices_only_df_new = justices_only_df.groupby(['Area','Speaker', 'Case Name', 'Latitude', 'Longitude'])['Word Count'].sum()\n",
    "\n",
    "justices_only_df_new = justices_only_df_new.reset_index()\n",
    "justices_only_df_new.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = justices_only_df_new.groupby(['Area', 'Speaker'])['Word Count'].sum().sort_values(ascending=True).plot(x='Speaker',y='Word Count', kind='barh', figsize=(50,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "justices_only_df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn the lat/lon columns into a new column, geometry, that is geometic points\n",
    "justices_only_df_new['geometry'] = justices_only_df_new.apply(lambda row: Point(row.Latitude, row.Longitude), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justices_only_df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "justices_only_df_new.rename(columns={'Area': 'area', 'Speaker': 'speaker', 'Case Name':'case_name', 'Latitude':'latitude','Longitude': 'longitude', 'Word Count':'word_count'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justices_only_df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justices_only_df_new = gpd.GeoDataFrame(justices_only_df_new)\n",
    "type(justices_only_df_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "justices_only_df_new.to_file('justices_only_df_new.json', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each area, who is the most frequent speaker?\n",
    "justices_only_df.plot(x='Speaker', y='Word Count', figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have a data frame (area_grouped_df) where we can look for most frequent words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert the Text column to a string so you can count \n",
    "#the most frequent words (there are some numbers in there)\n",
    "\n",
    "\n",
    "area_grouped_df['Text'] = area_grouped_df['Text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justices_only_df.groupby('Speaker')['Text'].nunique()\n",
    "wordcount = justices_only_df.groupby([\"Speaker\"]).sum().applymap(lambda words: Counter(re.findall(r\"\\b\\w{12,}\\b\",words.lower())).most_common())\n",
    "wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freqwords = area_grouped_df.head().applymap(lambda x:Counter(\" \".join(area_grouped_df[\"Text\"]).split()).most_common())\n",
    "# freqwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freqwords = area_grouped_df.applymap(lambda x: Counter(re.findall(r\"\\b\\w{5,}\\b\",x.lower())).most_common())\n",
    "\n",
    "# wordcount = df.groupby([\"speaker\"]).sum().applymap(lambda words: Counter(re.findall(r\"\\b\\w{12,}\\b\",words.lower())).most_common())\n",
    "\n",
    "\n",
    "freqwords = area_grouped_df.groupby([\"Area\"]).sum().applymap(lambda words: Counter(re.findall(r\"\\b\\w{5,}\\b\",words.lower())).most_common())\n",
    "freqwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(freq_words):\n",
    "    string = \"\"\n",
    "    for pair in freq_words[0:10]:\n",
    "        string += pair[0] + \": \" + str(pair[1]) + \" \"\n",
    "    return string\n",
    "freqwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freqwords = area_grouped_df.applymap(lambda x:Counter(\" \".join(area_grouped_df[\"Text\"]).split()).most_common())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unpack(freq_words):\n",
    "#     string = \"\"\n",
    "#     for pair in freq_words[0:10]:\n",
    "#         string += pair[0] + \": \" + str(pair[1]) + \" \"\n",
    "#     return string\n",
    "# freqwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions that semi-work:\n",
    "\n",
    "# Counter(\" \".join(df_all_info[\"Text\"]).split()).most_common()\n",
    "\n",
    "freqwords = area_grouped_df.applymap(lambda x: Counter(re.findall(r\"\\b\\w{5,}\\b\",x.lower())).most_common())\n",
    "\n",
    "#find the old df with just\n",
    "\n",
    "\n",
    "def unpack(freq_words):\n",
    "    string = \"\"\n",
    "    for pair in freq_words[0:10]:\n",
    "        string += pair[0] + \": \" + str(pair[1]) + \" \"\n",
    "    return string\n",
    "freqwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#THIS ONE WORKS\n",
    "# freqwords = supreme_words.groupby(['Docket Number']).sum().applymap(lambda x:Counter(\" \".join(df_all_info[\"Text\"]).split()).most_common())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#group by case and see what you want to get out\n",
    "#most frequent words per case. most frequent words per state\n",
    "#what case had the longest transcript?\n",
    "#pick a particular justice or two to track\n",
    "#which justice spoke the most from case to case\n",
    "#longest word count by any justice in each case and have that be printed as the pull quote\n",
    "#you could focus on ten cases in one sector\n",
    "#get a data frame and compress it and export it\n",
    "#groupby case, get a valuecount for each word in the text\n",
    "#groupby state, count the number of occurrences of speech for every justice, make that into df that has it grouped by\n",
    "#case and speaker\n",
    "#group down to the case and run things on the case and see what you can pull out of the case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's look at the geojson file. Eventually we'll have to merge it right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# geo_file = \"/Users/kaitlincough/Documents/Lede/thirkield/final_project_supreme_court/map_templatesUPDATED/states_data_all.geojson\"\n",
    "# states = gpd.read_file(geo_file)\n",
    "# states.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "geo_points = pd.read_csv('/Users/kaitlincough/Documents/Lede/thirkield/final_project_supreme_court/supreme_lat_lon.csv')\n",
    "geo_points = geo_points[['Docket Number', 'Latitude', 'Longitude']]\n",
    "\n",
    "# freqwords = freqwords.merge(geo_points, left_on='Docket Number', right_on='Docket Number', how='outer')\n",
    "\n",
    "# freqwords['geometry'] = freqwords.apply(lambda row: Point(row.Longitude, row.Latitude), axis=1)\n",
    "\n",
    "# #turn final data frame into geojson file\n",
    "# freqwords = gpd.GeoDataFrame(freqwords)\n",
    "# type(freqwords)\n",
    "# freqwords.to_file('WHATEVER.json', driver='GeoJSON')\n",
    "\n",
    "# must have lat/lon/docket number \n",
    "\n",
    "# #read in the csv file we made\n",
    "# geo_points = pd.read_csv('/Users/kaitlincough/Documents/Lede/thirkield/final_project_supreme_court/supreme_lat_lon.csv')\n",
    "# #only import these columns\n",
    "# geo_points = geo_points[['Docket Number', 'Latitude', 'Longitude']]\n",
    "\n",
    "#merge the data frame we made with the final data frame\n",
    "# freqwords = freqwords.merge(geo_points, left_on='Docket Number', right_on='Docket Number', how='outer')\n",
    "\n",
    "#turn the lat/lon columns into a new column, geometry, that is geometic points\n",
    "# freqwords['geometry'] = freqwords.apply(lambda row: Point(row.Longitude, row.Latitude), axis=1)\n",
    "\n",
    "#change the colors based on a definition\n",
    "# freqwords.loc[freqwords['word_count'] > 400, 'color'] = 'black'\n",
    "\n",
    "#change the filename in the html file\n",
    "#paste all of your json file into the points.json or wahtever it is in the maps template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqwords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqwords = freqwords.merge(geo_points, left_on='Docket Number', right_on='case_id', how='outer')\n",
    "\n",
    "# freqwords['geometry'] = freqwords.apply(lambda row: Point(row.Longitude, row.Latitude), axis=1)\n",
    "\n",
    "# #turn final data frame into geojson file\n",
    "# freqwords = gpd.GeoDataFrame(freqwords)\n",
    "# type(freqwords)\n",
    "# freqwords.to_file('WHATEVER.json', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#turn final data frame into geojson file\n",
    "final_data = gpd.GeoDataFrame(final_data)\n",
    "type(final_data)\n",
    "final_data.to_file('WHATEVER.json', driver='GeoJSON')\n",
    "\n",
    "# must have lat/lon/docket number \n",
    "\n",
    "#read in the csv file we made\n",
    "geo_points = pd.read_csv('/Users/kaitlincough/Documents/Lede/thirkield/final_project_supreme_court/supreme_lat_lon.csv')\n",
    "#only import these columns\n",
    "geo_points = geo_points[['Docket Number', 'Latitude', 'Longitude']]\n",
    "\n",
    "#merge the data frame we made with the final data frame\n",
    "final_data = final_data.merge(geo_points, left_on='Docket Number', right_on='Docket Number', how='outer')\n",
    "\n",
    "#turn the lat/lon columns into a new column, geometry, that is geometic points\n",
    "final_data['geometry'] = final_data.apply(lambda row: Point(row.Longitude, row.Latitude), axis=1)\n",
    "\n",
    "#change the colors based on a definition\n",
    "final_data.loc[final_data['word_count'] > 400, 'color'] = 'black'\n",
    "\n",
    "#change the filename in the html file\n",
    "#paste all of your json file into the points.json or wahtever it is in the maps template "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
